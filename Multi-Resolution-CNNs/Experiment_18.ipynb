{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# change the seed before anything else\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(7)\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Activation, merge, Flatten\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands = 60\n",
    "frames = 101\n",
    "feature_size = bands * frames\n",
    "num_channels = 3\n",
    "num_labels = 10\n",
    "data_dir = 'folds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all_folds(test_fold):\n",
    "    assert (type(test_fold) == int)\n",
    "    assert (test_fold > 0 and test_fold < 11)\n",
    "    subsequent_fold = False\n",
    "\n",
    "    train_set_range = list(range(1, 11))\n",
    "    train_set_range.remove(test_fold)\n",
    "    valid_fold = train_set_range.pop()\n",
    "\n",
    "    for k in train_set_range:\n",
    "        fold_name = 'fold' + str(k)\n",
    "        feature_file = os.path.join(data_dir, fold_name + '_x.npy')\n",
    "        labels_file = os.path.join(data_dir, fold_name + '_y.npy')\n",
    "        loaded_features = np.load(feature_file)\n",
    "        # flip the spectrogram for each channel\n",
    "        loaded_features = np.transpose(loaded_features, (0, 2, 1, 3))\n",
    "        loaded_labels = np.load(labels_file)\n",
    "        print(\"Adding \", fold_name, \"New Features: \", loaded_features.shape)\n",
    "\n",
    "        if subsequent_fold:\n",
    "            train_x_loaded = np.concatenate((train_x_loaded, loaded_features))\n",
    "            train_y_loaded = np.concatenate((train_y_loaded, loaded_labels))\n",
    "        else:\n",
    "            train_x_loaded = loaded_features\n",
    "            train_y_loaded = loaded_labels\n",
    "            subsequent_fold = True\n",
    "\n",
    "    # use the penultimate fold for validation\n",
    "    valid_fold_name = 'fold' + str(valid_fold)\n",
    "    feature_file = os.path.join(data_dir, valid_fold_name + '_x.npy')\n",
    "    labels_file = os.path.join(data_dir, valid_fold_name + '_y.npy')\n",
    "    valid_x = np.load(feature_file)\n",
    "    # flip the spectrogram for each channel\n",
    "    valid_x = np.transpose(valid_x, (0, 2, 1, 3))\n",
    "    valid_y = np.load(labels_file)\n",
    "\n",
    "    # and use the last fold for testing\n",
    "    test_fold_name = 'fold' + str(test_fold)\n",
    "    feature_file = os.path.join(data_dir, test_fold_name + '_x.npy')\n",
    "    labels_file = os.path.join(data_dir, test_fold_name + '_y.npy')\n",
    "    test_x = np.load(feature_file)\n",
    "    test_x = np.transpose(test_x, (0, 2, 1, 3))\n",
    "    test_y = np.load(labels_file)\n",
    "    return train_x_loaded, train_y_loaded, valid_x, valid_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y):\n",
    "    y_prob = model.predict(test_x, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    y_true = np.argmax(test_y, 1)\n",
    "\n",
    "    # evaluate the model\n",
    "    score, accuracy = model.evaluate(test_x, test_y, batch_size=32)\n",
    "    print(\"\\nAccuracy = {:.4f}\".format(accuracy))\n",
    "    print(\"\\nError Rate = {:.4f}\".format(1. - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # create model\n",
    "    \n",
    "    visible = Input(shape=(frames, bands, num_channels))\n",
    "    \n",
    "    # medium frequency features\n",
    "    # first filter\n",
    "    conv1 = Conv2D(1, kernel_size=(20, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool1 = MaxPooling2D(pool_size=(1, 2))(conv1)\n",
    "    dropout1 = Dropout(0.5)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # second filter\n",
    "    conv2 =Conv2D(1, kernel_size=(24, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool2 = MaxPooling2D(pool_size=(1, 2))(conv2)\n",
    "    dropout2 = Dropout(0.5)(conv2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # third filter\n",
    "    conv3 =Conv2D(1, kernel_size=(28, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool3 = MaxPooling2D(pool_size=(1, 2))(conv3)\n",
    "    dropout3 = Dropout(0.5)(conv3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # fourth filter\n",
    "    conv4 =Conv2D(1, kernel_size=(32, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool4 = MaxPooling2D(pool_size=(1, 2))(conv4)\n",
    "    dropout4 = Dropout(0.5)(conv4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    # fifth filter\n",
    "    conv5 =Conv2D(1, kernel_size=(36, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool5 = MaxPooling2D(pool_size=(1, 2))(conv5)\n",
    "    dropout5 = Dropout(0.5)(conv5)\n",
    "    flat5 = Flatten()(pool5)\n",
    "    \n",
    "    #long frequency features\n",
    "    # sixth filter\n",
    "    conv6 = Conv2D(1, kernel_size=(40, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool6 = MaxPooling2D(pool_size=(1, 2))(conv6)\n",
    "    dropout6 = Dropout(0.5)(conv6)\n",
    "    flat6 = Flatten()(pool6)\n",
    "    # seventh filter\n",
    "    conv7 = Conv2D(1, kernel_size=(44, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool7 = MaxPooling2D(pool_size=(1, 2))(conv7)\n",
    "    dropout7 = Dropout(0.5)(conv7)\n",
    "    flat7 = Flatten()(pool7)\n",
    "    # eighth filter\n",
    "    conv8 = Conv2D(1, kernel_size=(48, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool8 = MaxPooling2D(pool_size=(1, 2))(conv8)\n",
    "    dropout8 = Dropout(0.5)(conv8)\n",
    "    flat8 = Flatten()(pool8)\n",
    "    # ninth filter\n",
    "    conv9 = Conv2D(1, kernel_size=(52, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool9 = MaxPooling2D(pool_size=(1, 2))(conv9)\n",
    "    dropout9 = Dropout(0.5)(conv9)\n",
    "    flat9 = Flatten()(pool9)\n",
    "    # tenth filter\n",
    "    conv10 = Conv2D(1, kernel_size=(56, 4), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool10 = MaxPooling2D(pool_size=(1, 2))(conv10)\n",
    "    dropout10 = Dropout(0.5)(conv10)\n",
    "    flat10 = Flatten()(pool10)\n",
    "    \n",
    "    # medium temporal features\n",
    "    # 16\n",
    "    conv16 = Conv2D(1, kernel_size=(4, 20), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool16 = MaxPooling2D(pool_size=(2, 1))(conv16)\n",
    "    dropout16 = Dropout(0.5)(conv16)\n",
    "    flat16 = Flatten()(pool16)\n",
    "    # 17\n",
    "    conv17 = Conv2D(1, kernel_size=(4, 24), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool17 = MaxPooling2D(pool_size=(2, 1))(conv17)\n",
    "    dropout17 = Dropout(0.5)(conv17)\n",
    "    flat17 = Flatten()(pool17)\n",
    "    # 18\n",
    "    conv18 = Conv2D(1, kernel_size=(4, 28), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool18 = MaxPooling2D(pool_size=(2, 1))(conv18)\n",
    "    dropout18 = Dropout(0.5)(conv18)\n",
    "    flat18 = Flatten()(pool18)\n",
    "    # 19\n",
    "    conv19 = Conv2D(1, kernel_size=(4, 32), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool19 = MaxPooling2D(pool_size=(2, 1))(conv19)\n",
    "    dropout19 = Dropout(0.5)(conv19)\n",
    "    flat19 = Flatten()(pool19)\n",
    "    # 20\n",
    "    conv20 = Conv2D(1, kernel_size=(4, 36), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool20 = MaxPooling2D(pool_size=(2, 1))(conv20)\n",
    "    dropout20 = Dropout(0.5)(conv20)\n",
    "    flat20 = Flatten()(pool20)\n",
    "\n",
    "    #long temporal filters\n",
    "    # 21\n",
    "    conv21 = Conv2D(1, kernel_size=(4, 40), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool21 = MaxPooling2D(pool_size=(2, 1))(conv21)\n",
    "    dropout21 = Dropout(0.5)(conv21)\n",
    "    flat21 = Flatten()(pool21)\n",
    "    # 22\n",
    "    conv22 = Conv2D(1, kernel_size=(4, 44), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool22 = MaxPooling2D(pool_size=(2, 1))(conv22)\n",
    "    dropout22 = Dropout(0.5)(conv22)\n",
    "    flat22 = Flatten()(pool22)\n",
    "    # 23\n",
    "    conv23 = Conv2D(1, kernel_size=(4, 48), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool23 = MaxPooling2D(pool_size=(2, 1))(conv23)\n",
    "    dropout23 = Dropout(0.5)(conv23)\n",
    "    flat23 = Flatten()(pool23)\n",
    "    # 24\n",
    "    conv24 = Conv2D(1, kernel_size=(4, 52), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool24 = MaxPooling2D(pool_size=(2, 1))(conv24)\n",
    "    dropout24 = Dropout(0.5)(conv24)\n",
    "    flat24 = Flatten()(pool24)\n",
    "    # 25\n",
    "    conv25 = Conv2D(1, kernel_size=(4, 56), activation='relu', kernel_initializer='he_uniform')(visible)\n",
    "    pool25 = MaxPooling2D(pool_size=(2, 1))(conv25)\n",
    "    dropout25 = Dropout(0.5)(conv25)\n",
    "    flat25 = Flatten()(pool25)\n",
    "\n",
    "    \n",
    "    # merge filters\n",
    "    medium_filters = concatenate([flat1, flat2, flat3, flat4, flat5,\n",
    "                                  flat16, flat17, flat18, flat19, flat20])\n",
    "                           \n",
    "    long_filters = concatenate([flat6, flat7, flat8, flat9, flat10,\n",
    "                                flat21, flat22, flat23, flat24, flat25])\n",
    "    \n",
    "    merge = concatenate([medium_filters, long_filters])\n",
    "    \n",
    "    # interpretation layer\n",
    "    hidden1 = Dense(5000, activation='relu')(merge)\n",
    "    hidden2 = Dense(5000, activation='relu')(hidden1)\n",
    "    # prediction output\n",
    "    output = Dense(10, activation='softmax')(hidden2)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply scaling factor to a dataset - train, validation or test\n",
    "def do_scale(x4d, verbose = True):\n",
    "    \"\"\"Do scale on the input sequence data.\n",
    "\n",
    "    Args:\n",
    "      x34d: ndarray, input sequence data, shape: (n_clips, n_time, n_freq, channel)      \n",
    "      verbose: boolean\n",
    "\n",
    "    Returns:\n",
    "      Scaled input sequence data.\n",
    "    \"\"\"\n",
    "    t1 = time.time()    \n",
    "    (n_clips, n_time, n_freq, n_channel) = x4d.shape\n",
    "    x4d_scaled = np.zeros(x4d.shape)\n",
    "    for channel in range(n_channel):\n",
    "        x2d = x4d[:,:,:,channel].reshape((n_clips * n_time, n_freq))\n",
    "        x2d_scaled = scaler_list[channel].transform(x2d)\n",
    "        x3d_scaled = x2d_scaled.reshape((n_clips, n_time, n_freq))\n",
    "        x4d_scaled[:,:,:,channel] = x3d_scaled\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(\"Scaling time: %s\" % (time.time() - t1,))\n",
    "\n",
    "    return x4d_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening fold: 1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.5900306701660156\n",
      "Scaling time: 0.3473660945892334\n",
      "Scaling time: 0.4064607620239258\n",
      "training model...hold tight\n",
      "Train on 7021 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "7021/7021 [==============================] - 43s - loss: 1.4548 - acc: 0.5021 - val_loss: 1.3717 - val_acc: 0.4767\n",
      "Epoch 2/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.9230 - acc: 0.6821 - val_loss: 1.3016 - val_acc: 0.5603\n",
      "Epoch 3/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.7076 - acc: 0.7637 - val_loss: 1.3984 - val_acc: 0.4958\n",
      "Epoch 4/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.5470 - acc: 0.8214 - val_loss: 1.2836 - val_acc: 0.5890\n",
      "Epoch 5/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.4116 - acc: 0.8749 - val_loss: 1.3106 - val_acc: 0.5723\n",
      "Epoch 6/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.3177 - acc: 0.9046 - val_loss: 1.5034 - val_acc: 0.5627\n",
      "Epoch 7/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.2399 - acc: 0.9356 - val_loss: 1.5188 - val_acc: 0.5830\n",
      "Epoch 8/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.1743 - acc: 0.9566 - val_loss: 1.6060 - val_acc: 0.5759\n",
      "Epoch 9/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.1326 - acc: 0.9702 - val_loss: 1.5771 - val_acc: 0.6380\n",
      "Epoch 10/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.1072 - acc: 0.9764 - val_loss: 1.6374 - val_acc: 0.5783\n",
      "Epoch 11/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0698 - acc: 0.9892 - val_loss: 1.8291 - val_acc: 0.6177\n",
      "Epoch 12/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0536 - acc: 0.9915 - val_loss: 1.8864 - val_acc: 0.5938\n",
      "Epoch 13/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0434 - acc: 0.9922 - val_loss: 1.6734 - val_acc: 0.6225\n",
      "Epoch 14/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0329 - acc: 0.9956 - val_loss: 1.9035 - val_acc: 0.5950\n",
      "Epoch 15/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0297 - acc: 0.9954 - val_loss: 1.8516 - val_acc: 0.6105\n",
      "Epoch 16/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0223 - acc: 0.9967 - val_loss: 1.8641 - val_acc: 0.6057\n",
      "Epoch 17/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0207 - acc: 0.9972 - val_loss: 1.9624 - val_acc: 0.6117\n",
      "Epoch 18/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0179 - acc: 0.9974 - val_loss: 1.8744 - val_acc: 0.6201\n",
      "Epoch 19/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0155 - acc: 0.9974 - val_loss: 1.9273 - val_acc: 0.6129\n",
      "Epoch 20/150\n",
      "7021/7021 [==============================] - 30s - loss: 0.0139 - acc: 0.9980 - val_loss: 2.1054 - val_acc: 0.5938\n",
      "Epoch 21/150\n",
      "7021/7021 [==============================] - 23s - loss: 0.0127 - acc: 0.9976 - val_loss: 2.1044 - val_acc: 0.6010\n",
      "Epoch 22/150\n",
      "7021/7021 [==============================] - 13s - loss: 0.0131 - acc: 0.9969 - val_loss: 2.0612 - val_acc: 0.6069\n",
      "Epoch 23/150\n",
      "7021/7021 [==============================] - 13s - loss: 0.0149 - acc: 0.9972 - val_loss: 2.0614 - val_acc: 0.5938\n",
      "Epoch 24/150\n",
      "7021/7021 [==============================] - 13s - loss: 0.0147 - acc: 0.9972 - val_loss: 1.8648 - val_acc: 0.6272\n",
      "Epoch 25/150\n",
      "7021/7021 [==============================] - 14s - loss: 0.0104 - acc: 0.9983 - val_loss: 2.1301 - val_acc: 0.5986\n",
      "Epoch 00024: early stopping\n",
      "training time: 701.8511247634888\n",
      "800/869 [==========================>...] - ETA: 0s\n",
      "Accuracy = 0.5397\n",
      "\n",
      "Error Rate = 0.4603\n",
      "training time: 1.1043860912322998\n",
      "opening fold: 2\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.211374282836914\n",
      "Scaling time: 0.29784488677978516\n",
      "Scaling time: 0.3115355968475342\n",
      "training model...hold tight\n",
      "Train on 7003 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "7003/7003 [==============================] - 17s - loss: 1.4524 - acc: 0.4936 - val_loss: 1.8588 - val_acc: 0.3393\n",
      "Epoch 2/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.9110 - acc: 0.6896 - val_loss: 1.7716 - val_acc: 0.4194\n",
      "Epoch 3/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.6859 - acc: 0.7721 - val_loss: 2.5623 - val_acc: 0.3154\n",
      "Epoch 4/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.5453 - acc: 0.8271 - val_loss: 1.5048 - val_acc: 0.5149\n",
      "Epoch 5/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.4077 - acc: 0.8716 - val_loss: 1.3306 - val_acc: 0.6022\n",
      "Epoch 6/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.2987 - acc: 0.9139 - val_loss: 1.2838 - val_acc: 0.6272\n",
      "Epoch 7/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.2222 - acc: 0.9402 - val_loss: 1.3431 - val_acc: 0.6213\n",
      "Epoch 8/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.1675 - acc: 0.9590 - val_loss: 1.4088 - val_acc: 0.6260\n",
      "Epoch 9/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.1204 - acc: 0.9746 - val_loss: 1.4430 - val_acc: 0.6356\n",
      "Epoch 10/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0867 - acc: 0.9832 - val_loss: 1.3550 - val_acc: 0.6691\n",
      "Epoch 11/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0662 - acc: 0.9871 - val_loss: 1.6871 - val_acc: 0.6022\n",
      "Epoch 12/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0544 - acc: 0.9909 - val_loss: 3.4892 - val_acc: 0.4898\n",
      "Epoch 13/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0545 - acc: 0.9911 - val_loss: 1.5740 - val_acc: 0.6260\n",
      "Epoch 14/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0244 - acc: 0.9981 - val_loss: 1.5903 - val_acc: 0.6332\n",
      "Epoch 15/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0306 - acc: 0.9956 - val_loss: 1.5645 - val_acc: 0.6583\n",
      "Epoch 16/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0175 - acc: 0.9983 - val_loss: 1.6363 - val_acc: 0.6476\n",
      "Epoch 17/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0123 - acc: 0.9991 - val_loss: 1.6200 - val_acc: 0.6691\n",
      "Epoch 18/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0194 - acc: 0.9969 - val_loss: 1.7740 - val_acc: 0.6105\n",
      "Epoch 19/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0132 - acc: 0.9983 - val_loss: 1.7072 - val_acc: 0.6487\n",
      "Epoch 20/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0099 - acc: 0.9984 - val_loss: 1.6856 - val_acc: 0.6655\n",
      "Epoch 21/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0085 - acc: 0.9993 - val_loss: 1.7196 - val_acc: 0.6487\n",
      "Epoch 22/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0063 - acc: 0.9997 - val_loss: 1.7418 - val_acc: 0.6607\n",
      "Epoch 23/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0062 - acc: 0.9994 - val_loss: 1.7303 - val_acc: 0.6583\n",
      "Epoch 24/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0061 - acc: 0.9997 - val_loss: 1.7588 - val_acc: 0.6583\n",
      "Epoch 25/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0048 - acc: 1.0000 - val_loss: 1.8085 - val_acc: 0.6440\n",
      "Epoch 26/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0049 - acc: 0.9997 - val_loss: 1.7883 - val_acc: 0.6464\n",
      "Epoch 27/150\n",
      "7003/7003 [==============================] - 13s - loss: 0.0040 - acc: 1.0000 - val_loss: 1.8308 - val_acc: 0.6523\n",
      "Epoch 00026: early stopping\n",
      "training time: 374.1491048336029\n",
      "887/887 [==============================] - 0s     \n",
      "\n",
      "Accuracy = 0.5964\n",
      "\n",
      "Error Rate = 0.4036\n",
      "training time: 1.0623598098754883\n",
      "opening fold: 3\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.276215076446533\n",
      "Scaling time: 0.2955772876739502\n",
      "Scaling time: 0.3192610740661621\n",
      "training model...hold tight\n",
      "Train on 6965 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "6965/6965 [==============================] - 17s - loss: 1.5102 - acc: 0.4800 - val_loss: 1.4555 - val_acc: 0.4683\n",
      "Epoch 2/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.9369 - acc: 0.6747 - val_loss: 1.4604 - val_acc: 0.4385\n",
      "Epoch 3/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.6906 - acc: 0.7723 - val_loss: 1.4362 - val_acc: 0.4516\n",
      "Epoch 4/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.5311 - acc: 0.8286 - val_loss: 1.3051 - val_acc: 0.5926\n",
      "Epoch 5/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.4061 - acc: 0.8719 - val_loss: 3.0777 - val_acc: 0.4277\n",
      "Epoch 6/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.3510 - acc: 0.8978 - val_loss: 1.6534 - val_acc: 0.5400\n",
      "Epoch 7/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.2342 - acc: 0.9342 - val_loss: 1.8760 - val_acc: 0.4839\n",
      "Epoch 8/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.1821 - acc: 0.9526 - val_loss: 1.3323 - val_acc: 0.6798\n",
      "Epoch 9/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.1295 - acc: 0.9681 - val_loss: 1.3793 - val_acc: 0.6487\n",
      "Epoch 10/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.1005 - acc: 0.9772 - val_loss: 1.3807 - val_acc: 0.6631\n",
      "Epoch 11/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0747 - acc: 0.9826 - val_loss: 1.5777 - val_acc: 0.6332\n",
      "Epoch 12/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0648 - acc: 0.9855 - val_loss: 1.4549 - val_acc: 0.6619\n",
      "Epoch 13/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0540 - acc: 0.9878 - val_loss: 1.5724 - val_acc: 0.6332\n",
      "Epoch 14/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0384 - acc: 0.9930 - val_loss: 1.6782 - val_acc: 0.6320\n",
      "Epoch 15/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0288 - acc: 0.9945 - val_loss: 1.6630 - val_acc: 0.6511\n",
      "Epoch 16/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0301 - acc: 0.9934 - val_loss: 1.8827 - val_acc: 0.6225\n",
      "Epoch 17/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0371 - acc: 0.9918 - val_loss: 3.0762 - val_acc: 0.5675\n",
      "Epoch 18/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0534 - acc: 0.9881 - val_loss: 1.8407 - val_acc: 0.6177\n",
      "Epoch 19/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0301 - acc: 0.9925 - val_loss: 1.5818 - val_acc: 0.6679\n",
      "Epoch 20/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0190 - acc: 0.9964 - val_loss: 1.6838 - val_acc: 0.6523\n",
      "Epoch 21/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0181 - acc: 0.9955 - val_loss: 2.1129 - val_acc: 0.6165\n",
      "Epoch 22/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0175 - acc: 0.9963 - val_loss: 1.7292 - val_acc: 0.6416\n",
      "Epoch 23/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0138 - acc: 0.9971 - val_loss: 1.7031 - val_acc: 0.6691\n",
      "Epoch 24/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0167 - acc: 0.9963 - val_loss: 1.7627 - val_acc: 0.6464\n",
      "Epoch 25/150\n",
      "6965/6965 [==============================] - 13s - loss: 0.0152 - acc: 0.9967 - val_loss: 1.7612 - val_acc: 0.6511\n",
      "Epoch 00024: early stopping\n",
      "training time: 347.2583827972412\n",
      "896/925 [============================>.] - ETA: 0s\n",
      "Accuracy = 0.5254\n",
      "\n",
      "Error Rate = 0.4746\n",
      "training time: 1.1142733097076416\n",
      "opening fold: 4\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.2932612895965576\n",
      "Scaling time: 0.2907559871673584\n",
      "Scaling time: 0.3341386318206787\n",
      "training model...hold tight\n",
      "Train on 6900 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "6900/6900 [==============================] - 17s - loss: 1.4223 - acc: 0.4964 - val_loss: 1.3147 - val_acc: 0.5376\n",
      "Epoch 2/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.8940 - acc: 0.6942 - val_loss: 1.3235 - val_acc: 0.5484\n",
      "Epoch 3/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.6595 - acc: 0.7842 - val_loss: 1.2180 - val_acc: 0.6165\n",
      "Epoch 4/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.4893 - acc: 0.8472 - val_loss: 1.1489 - val_acc: 0.6786\n",
      "Epoch 5/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.3637 - acc: 0.8919 - val_loss: 1.2745 - val_acc: 0.6583\n",
      "Epoch 6/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.2641 - acc: 0.9233 - val_loss: 1.4658 - val_acc: 0.6081\n",
      "Epoch 7/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.2015 - acc: 0.9470 - val_loss: 1.2348 - val_acc: 0.6464\n",
      "Epoch 8/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.1539 - acc: 0.9630 - val_loss: 1.3469 - val_acc: 0.6965\n",
      "Epoch 9/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.1026 - acc: 0.9796 - val_loss: 1.4274 - val_acc: 0.6726\n",
      "Epoch 10/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0777 - acc: 0.9851 - val_loss: 1.4075 - val_acc: 0.6595\n",
      "Epoch 11/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0564 - acc: 0.9904 - val_loss: 1.5994 - val_acc: 0.6523\n",
      "Epoch 12/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0423 - acc: 0.9941 - val_loss: 1.4536 - val_acc: 0.6822\n",
      "Epoch 13/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0398 - acc: 0.9925 - val_loss: 1.4620 - val_acc: 0.6918\n",
      "Epoch 14/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0285 - acc: 0.9948 - val_loss: 1.5040 - val_acc: 0.6965\n",
      "Epoch 15/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0253 - acc: 0.9962 - val_loss: 1.4652 - val_acc: 0.6882\n",
      "Epoch 16/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0254 - acc: 0.9948 - val_loss: 1.6135 - val_acc: 0.6356\n",
      "Epoch 17/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0197 - acc: 0.9965 - val_loss: 1.6330 - val_acc: 0.6774\n",
      "Epoch 18/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0237 - acc: 0.9955 - val_loss: 1.5675 - val_acc: 0.6726\n",
      "Epoch 19/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0206 - acc: 0.9964 - val_loss: 1.6725 - val_acc: 0.6834\n",
      "Epoch 20/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0173 - acc: 0.9965 - val_loss: 1.5735 - val_acc: 0.6762\n",
      "Epoch 21/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0150 - acc: 0.9980 - val_loss: 1.6331 - val_acc: 0.6643\n",
      "Epoch 22/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0140 - acc: 0.9972 - val_loss: 1.7694 - val_acc: 0.6296\n",
      "Epoch 23/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0181 - acc: 0.9959 - val_loss: 1.8319 - val_acc: 0.6153\n",
      "Epoch 24/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0115 - acc: 0.9984 - val_loss: 1.7080 - val_acc: 0.6404\n",
      "Epoch 25/150\n",
      "6900/6900 [==============================] - 13s - loss: 0.0147 - acc: 0.9971 - val_loss: 1.6862 - val_acc: 0.6655\n",
      "Epoch 00024: early stopping\n",
      "training time: 337.21838998794556\n",
      "928/990 [===========================>..] - ETA: 0s\n",
      "Accuracy = 0.6404\n",
      "\n",
      "Error Rate = 0.3596\n",
      "training time: 1.1271462440490723\n",
      "opening fold: 5\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.34190034866333\n",
      "Scaling time: 0.3150954246520996\n",
      "Scaling time: 0.3200104236602783\n",
      "training model...hold tight\n",
      "Train on 6954 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "6954/6954 [==============================] - 17s - loss: 1.4414 - acc: 0.4964 - val_loss: 1.3763 - val_acc: 0.5149\n",
      "Epoch 2/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.9172 - acc: 0.6836 - val_loss: 1.2808 - val_acc: 0.5699\n",
      "Epoch 3/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.7046 - acc: 0.7634 - val_loss: 1.2669 - val_acc: 0.6093\n",
      "Epoch 4/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.5381 - acc: 0.8224 - val_loss: 1.2904 - val_acc: 0.6057\n",
      "Epoch 5/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.4155 - acc: 0.8758 - val_loss: 1.4049 - val_acc: 0.5484\n",
      "Epoch 6/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.3230 - acc: 0.9032 - val_loss: 1.4150 - val_acc: 0.5651\n",
      "Epoch 7/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.2488 - acc: 0.9316 - val_loss: 1.3126 - val_acc: 0.6129\n",
      "Epoch 8/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.1794 - acc: 0.9524 - val_loss: 1.3526 - val_acc: 0.6428\n",
      "Epoch 9/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.1356 - acc: 0.9687 - val_loss: 1.5101 - val_acc: 0.6368\n",
      "Epoch 10/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0983 - acc: 0.9800 - val_loss: 1.4732 - val_acc: 0.6667\n",
      "Epoch 11/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0730 - acc: 0.9843 - val_loss: 1.5580 - val_acc: 0.6069\n",
      "Epoch 12/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0622 - acc: 0.9889 - val_loss: 1.5207 - val_acc: 0.6571\n",
      "Epoch 13/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0479 - acc: 0.9907 - val_loss: 1.5936 - val_acc: 0.6440\n",
      "Epoch 14/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0441 - acc: 0.9919 - val_loss: 1.5514 - val_acc: 0.6476\n",
      "Epoch 15/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0311 - acc: 0.9940 - val_loss: 1.5918 - val_acc: 0.6655\n",
      "Epoch 16/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0213 - acc: 0.9965 - val_loss: 1.6567 - val_acc: 0.6583\n",
      "Epoch 17/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0250 - acc: 0.9954 - val_loss: 1.6934 - val_acc: 0.6260\n",
      "Epoch 18/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0195 - acc: 0.9967 - val_loss: 1.7030 - val_acc: 0.6535\n",
      "Epoch 19/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0180 - acc: 0.9965 - val_loss: 1.7547 - val_acc: 0.6308\n",
      "Epoch 20/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0148 - acc: 0.9978 - val_loss: 1.7788 - val_acc: 0.6392\n",
      "Epoch 21/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0170 - acc: 0.9970 - val_loss: 1.6715 - val_acc: 0.6583\n",
      "Epoch 22/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0165 - acc: 0.9965 - val_loss: 1.8502 - val_acc: 0.6607\n",
      "Epoch 23/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0146 - acc: 0.9967 - val_loss: 1.7972 - val_acc: 0.6523\n",
      "Epoch 24/150\n",
      "6954/6954 [==============================] - 13s - loss: 0.0128 - acc: 0.9971 - val_loss: 1.8310 - val_acc: 0.6428\n",
      "Epoch 00023: early stopping\n",
      "training time: 331.0166687965393\n",
      "928/936 [============================>.] - ETA: 0s\n",
      "Accuracy = 0.6859\n",
      "\n",
      "Error Rate = 0.3141\n",
      "training time: 1.0895183086395264\n",
      "opening fold: 6\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.297276258468628\n",
      "Scaling time: 0.29703545570373535\n",
      "Scaling time: 0.29128241539001465\n",
      "training model...hold tight\n",
      "Train on 7067 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "7067/7067 [==============================] - 18s - loss: 1.5314 - acc: 0.4732 - val_loss: 1.3391 - val_acc: 0.4504\n",
      "Epoch 2/150\n",
      "7067/7067 [==============================] - 14s - loss: 0.9350 - acc: 0.6728 - val_loss: 1.2675 - val_acc: 0.5651\n",
      "Epoch 3/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.6951 - acc: 0.7657 - val_loss: 1.2520 - val_acc: 0.5998\n",
      "Epoch 4/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.5424 - acc: 0.8217 - val_loss: 1.1539 - val_acc: 0.6416\n",
      "Epoch 5/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.4004 - acc: 0.8780 - val_loss: 1.1784 - val_acc: 0.6332\n",
      "Epoch 6/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.3069 - acc: 0.9103 - val_loss: 1.1819 - val_acc: 0.6607\n",
      "Epoch 7/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.2285 - acc: 0.9369 - val_loss: 1.3004 - val_acc: 0.6296\n",
      "Epoch 8/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.1682 - acc: 0.9573 - val_loss: 1.3993 - val_acc: 0.6189\n",
      "Epoch 9/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.1245 - acc: 0.9721 - val_loss: 1.3263 - val_acc: 0.6440\n",
      "Epoch 10/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0940 - acc: 0.9809 - val_loss: 1.2325 - val_acc: 0.6870\n",
      "Epoch 11/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0751 - acc: 0.9843 - val_loss: 1.3005 - val_acc: 0.6703\n",
      "Epoch 12/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0595 - acc: 0.9874 - val_loss: 1.4388 - val_acc: 0.6822\n",
      "Epoch 13/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0578 - acc: 0.9883 - val_loss: 1.4444 - val_acc: 0.6559\n",
      "Epoch 14/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0436 - acc: 0.9911 - val_loss: 1.4176 - val_acc: 0.6762\n",
      "Epoch 15/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0356 - acc: 0.9928 - val_loss: 1.4467 - val_acc: 0.6786\n",
      "Epoch 16/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0289 - acc: 0.9932 - val_loss: 1.4181 - val_acc: 0.6858\n",
      "Epoch 17/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0248 - acc: 0.9955 - val_loss: 1.4658 - val_acc: 0.6906\n",
      "Epoch 18/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0202 - acc: 0.9969 - val_loss: 1.4691 - val_acc: 0.6762\n",
      "Epoch 19/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0199 - acc: 0.9962 - val_loss: 1.5060 - val_acc: 0.6810\n",
      "Epoch 20/150\n",
      "7067/7067 [==============================] - 14s - loss: 0.0183 - acc: 0.9960 - val_loss: 1.5028 - val_acc: 0.6643\n",
      "Epoch 21/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0158 - acc: 0.9975 - val_loss: 1.5238 - val_acc: 0.6762\n",
      "Epoch 22/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0127 - acc: 0.9984 - val_loss: 1.6217 - val_acc: 0.6535\n",
      "Epoch 23/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0232 - acc: 0.9952 - val_loss: 1.5475 - val_acc: 0.6655\n",
      "Epoch 24/150\n",
      "7067/7067 [==============================] - 14s - loss: 0.0165 - acc: 0.9962 - val_loss: 1.5149 - val_acc: 0.6774\n",
      "Epoch 25/150\n",
      "7067/7067 [==============================] - 13s - loss: 0.0123 - acc: 0.9976 - val_loss: 1.5588 - val_acc: 0.6679\n",
      "Epoch 00024: early stopping\n",
      "training time: 350.8441512584686\n",
      "800/823 [============================>.] - ETA: 0s\n",
      "Accuracy = 0.6294\n",
      "\n",
      "Error Rate = 0.3706\n",
      "training time: 0.946845531463623\n",
      "opening fold: 7\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.467597246170044\n",
      "Scaling time: 0.2962062358856201\n",
      "Scaling time: 0.2953603267669678\n",
      "training model...hold tight\n",
      "Train on 7052 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "7052/7052 [==============================] - 18s - loss: 1.4375 - acc: 0.4999 - val_loss: 1.4479 - val_acc: 0.4803\n",
      "Epoch 2/150\n",
      "7052/7052 [==============================] - 13s - loss: 0.9129 - acc: 0.6859 - val_loss: 1.3657 - val_acc: 0.5317\n",
      "Epoch 3/150\n",
      "7052/7052 [==============================] - 13s - loss: 0.6970 - acc: 0.7696 - val_loss: 1.3076 - val_acc: 0.5675\n",
      "Epoch 4/150\n",
      "7052/7052 [==============================] - 13s - loss: 0.5381 - acc: 0.8203 - val_loss: 1.4159 - val_acc: 0.5544\n",
      "Epoch 5/150\n",
      "7052/7052 [==============================] - 13s - loss: 0.4103 - acc: 0.8748 - val_loss: 1.2873 - val_acc: 0.6057\n",
      "Epoch 6/150\n",
      "7052/7052 [==============================] - 13s - loss: 0.3074 - acc: 0.9088 - val_loss: 1.3666 - val_acc: 0.6189\n",
      "Epoch 7/150\n",
      "7052/7052 [==============================] - 13s - loss: 0.2322 - acc: 0.9375 - val_loss: 1.4425 - val_acc: 0.5962\n",
      "Epoch 8/150\n",
      "7052/7052 [==============================] - 14s - loss: 0.1683 - acc: 0.9593 - val_loss: 1.3731 - val_acc: 0.6189\n",
      "Epoch 9/150\n",
      "7052/7052 [==============================] - 15s - loss: 0.1202 - acc: 0.9725 - val_loss: 1.4971 - val_acc: 0.6260\n",
      "Epoch 10/150\n",
      "7052/7052 [==============================] - 31s - loss: 0.0953 - acc: 0.9807 - val_loss: 1.4724 - val_acc: 0.6428\n",
      "Epoch 11/150\n",
      "7052/7052 [==============================] - 31s - loss: 0.0657 - acc: 0.9881 - val_loss: 1.5090 - val_acc: 0.6249\n",
      "Epoch 12/150\n",
      "7052/7052 [==============================] - 31s - loss: 0.0481 - acc: 0.9928 - val_loss: 1.4705 - val_acc: 0.6487\n",
      "Epoch 13/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0410 - acc: 0.9932 - val_loss: 1.5107 - val_acc: 0.6499\n",
      "Epoch 14/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0300 - acc: 0.9957 - val_loss: 1.5914 - val_acc: 0.6583\n",
      "Epoch 15/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0239 - acc: 0.9963 - val_loss: 1.5517 - val_acc: 0.6583\n",
      "Epoch 16/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0182 - acc: 0.9973 - val_loss: 1.5697 - val_acc: 0.6714\n",
      "Epoch 17/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0161 - acc: 0.9973 - val_loss: 1.5889 - val_acc: 0.6631\n",
      "Epoch 18/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0161 - acc: 0.9982 - val_loss: 1.5789 - val_acc: 0.6870\n",
      "Epoch 19/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0135 - acc: 0.9979 - val_loss: 1.6988 - val_acc: 0.6464\n",
      "Epoch 20/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0109 - acc: 0.9983 - val_loss: 1.6577 - val_acc: 0.6667\n",
      "Epoch 21/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0109 - acc: 0.9983 - val_loss: 1.6790 - val_acc: 0.6595\n",
      "Epoch 22/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0081 - acc: 0.9991 - val_loss: 1.6783 - val_acc: 0.6691\n",
      "Epoch 23/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0079 - acc: 0.9991 - val_loss: 1.6897 - val_acc: 0.6810\n",
      "Epoch 24/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0076 - acc: 0.9991 - val_loss: 1.7269 - val_acc: 0.6511\n",
      "Epoch 25/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0068 - acc: 0.9991 - val_loss: 1.7132 - val_acc: 0.6762\n",
      "Epoch 26/150\n",
      "7052/7052 [==============================] - 30s - loss: 0.0065 - acc: 0.9991 - val_loss: 1.7437 - val_acc: 0.6667\n",
      "Epoch 00025: early stopping\n",
      "training time: 654.9547519683838\n",
      "832/838 [============================>.] - ETA: 0s\n",
      "Accuracy = 0.6850\n",
      "\n",
      "Error Rate = 0.3150\n",
      "training time: 1.7320139408111572\n",
      "opening fold: 8\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold9 New Features:  (816, 101, 60, 3)\n",
      "Scaling time: 2.5168538093566895\n",
      "Scaling time: 0.36272573471069336\n",
      "Scaling time: 0.30870914459228516\n",
      "training model...hold tight\n",
      "Train on 7084 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "7084/7084 [==============================] - 35s - loss: 1.4674 - acc: 0.4999 - val_loss: 1.3583 - val_acc: 0.5472\n",
      "Epoch 2/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.9057 - acc: 0.6793 - val_loss: 1.8290 - val_acc: 0.4182\n",
      "Epoch 3/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.6774 - acc: 0.7799 - val_loss: 1.9687 - val_acc: 0.4636\n",
      "Epoch 4/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.5191 - acc: 0.8337 - val_loss: 1.5059 - val_acc: 0.5544\n",
      "Epoch 5/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.4124 - acc: 0.8737 - val_loss: 1.2366 - val_acc: 0.7001\n",
      "Epoch 6/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.2973 - acc: 0.9053 - val_loss: 1.4179 - val_acc: 0.5866\n",
      "Epoch 7/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.2186 - acc: 0.9418 - val_loss: 1.5937 - val_acc: 0.5806\n",
      "Epoch 8/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.1867 - acc: 0.9510 - val_loss: 1.5133 - val_acc: 0.6201\n",
      "Epoch 9/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.1346 - acc: 0.9695 - val_loss: 1.5384 - val_acc: 0.6356\n",
      "Epoch 10/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.1009 - acc: 0.9780 - val_loss: 1.5213 - val_acc: 0.6332\n",
      "Epoch 11/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0772 - acc: 0.9816 - val_loss: 1.4407 - val_acc: 0.6703\n",
      "Epoch 12/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0540 - acc: 0.9898 - val_loss: 1.5362 - val_acc: 0.6452\n",
      "Epoch 13/150\n",
      "7084/7084 [==============================] - 31s - loss: 0.0464 - acc: 0.9907 - val_loss: 1.6094 - val_acc: 0.6260\n",
      "Epoch 14/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0335 - acc: 0.9945 - val_loss: 1.7510 - val_acc: 0.6452\n",
      "Epoch 15/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0281 - acc: 0.9951 - val_loss: 1.6997 - val_acc: 0.6308\n",
      "Epoch 16/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0286 - acc: 0.9939 - val_loss: 1.6815 - val_acc: 0.6380\n",
      "Epoch 17/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0318 - acc: 0.9927 - val_loss: 1.6357 - val_acc: 0.6476\n",
      "Epoch 18/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0196 - acc: 0.9965 - val_loss: 1.7113 - val_acc: 0.6332\n",
      "Epoch 19/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0170 - acc: 0.9970 - val_loss: 1.7786 - val_acc: 0.6464\n",
      "Epoch 20/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0182 - acc: 0.9962 - val_loss: 1.7130 - val_acc: 0.6452\n",
      "Epoch 21/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0135 - acc: 0.9976 - val_loss: 1.7530 - val_acc: 0.6428\n",
      "Epoch 22/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0131 - acc: 0.9973 - val_loss: 1.8218 - val_acc: 0.6368\n",
      "Epoch 23/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0154 - acc: 0.9968 - val_loss: 1.7748 - val_acc: 0.6487\n",
      "Epoch 24/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0117 - acc: 0.9976 - val_loss: 1.8841 - val_acc: 0.6260\n",
      "Epoch 25/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0104 - acc: 0.9983 - val_loss: 1.8469 - val_acc: 0.6523\n",
      "Epoch 26/150\n",
      "7084/7084 [==============================] - 30s - loss: 0.0100 - acc: 0.9977 - val_loss: 1.8347 - val_acc: 0.6404\n",
      "Epoch 00025: early stopping\n",
      "training time: 806.4670841693878\n",
      "768/806 [===========================>..] - ETA: 0s\n",
      "Accuracy = 0.6365\n",
      "\n",
      "Error Rate = 0.3635\n",
      "training time: 1.3272998332977295\n",
      "opening fold: 9\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Scaling time: 2.197817087173462\n",
      "Scaling time: 0.2937140464782715\n",
      "Scaling time: 0.2823019027709961\n",
      "training model...hold tight\n",
      "Train on 7074 samples, validate on 837 samples\n",
      "Epoch 1/150\n",
      "7074/7074 [==============================] - 17s - loss: 1.4352 - acc: 0.4986 - val_loss: 1.3557 - val_acc: 0.4803\n",
      "Epoch 2/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.9024 - acc: 0.6924 - val_loss: 1.2146 - val_acc: 0.5615\n",
      "Epoch 3/150\n",
      "7074/7074 [==============================] - 12s - loss: 0.6852 - acc: 0.7748 - val_loss: 1.2565 - val_acc: 0.6237\n",
      "Epoch 4/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.5172 - acc: 0.8381 - val_loss: 1.2253 - val_acc: 0.6476\n",
      "Epoch 5/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.3881 - acc: 0.8846 - val_loss: 1.2256 - val_acc: 0.6356\n",
      "Epoch 6/150\n",
      "7074/7074 [==============================] - 12s - loss: 0.2889 - acc: 0.9207 - val_loss: 1.3536 - val_acc: 0.6033\n",
      "Epoch 7/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.2141 - acc: 0.9449 - val_loss: 1.5297 - val_acc: 0.5986\n",
      "Epoch 8/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.1630 - acc: 0.9597 - val_loss: 1.4202 - val_acc: 0.6284\n",
      "Epoch 9/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.1138 - acc: 0.9754 - val_loss: 1.3207 - val_acc: 0.7049\n",
      "Epoch 10/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0861 - acc: 0.9823 - val_loss: 1.4328 - val_acc: 0.6714\n",
      "Epoch 11/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0594 - acc: 0.9904 - val_loss: 1.4969 - val_acc: 0.6822\n",
      "Epoch 12/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0453 - acc: 0.9942 - val_loss: 1.4994 - val_acc: 0.6858\n",
      "Epoch 13/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0366 - acc: 0.9942 - val_loss: 1.6432 - val_acc: 0.6511\n",
      "Epoch 14/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0317 - acc: 0.9952 - val_loss: 1.6040 - val_acc: 0.6703\n",
      "Epoch 15/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0248 - acc: 0.9959 - val_loss: 1.5497 - val_acc: 0.6858\n",
      "Epoch 16/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0214 - acc: 0.9972 - val_loss: 1.6302 - val_acc: 0.6762\n",
      "Epoch 17/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0204 - acc: 0.9969 - val_loss: 1.6561 - val_acc: 0.6571\n",
      "Epoch 18/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0164 - acc: 0.9980 - val_loss: 1.6521 - val_acc: 0.6918\n",
      "Epoch 19/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0146 - acc: 0.9979 - val_loss: 1.7393 - val_acc: 0.6595\n",
      "Epoch 20/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0135 - acc: 0.9980 - val_loss: 1.7570 - val_acc: 0.6726\n",
      "Epoch 21/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0109 - acc: 0.9986 - val_loss: 1.6938 - val_acc: 0.6894\n",
      "Epoch 22/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0108 - acc: 0.9984 - val_loss: 1.7087 - val_acc: 0.6798\n",
      "Epoch 23/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0108 - acc: 0.9984 - val_loss: 1.6586 - val_acc: 0.6965\n",
      "Epoch 00022: early stopping\n",
      "training time: 313.43211913108826\n",
      "800/816 [============================>.] - ETA: 0s\n",
      "Accuracy = 0.6691\n",
      "\n",
      "Error Rate = 0.3309\n",
      "training time: 0.968015193939209\n",
      "opening fold: 10\n",
      "Adding  fold1 New Features:  (869, 101, 60, 3)\n",
      "Adding  fold2 New Features:  (887, 101, 60, 3)\n",
      "Adding  fold3 New Features:  (925, 101, 60, 3)\n",
      "Adding  fold4 New Features:  (990, 101, 60, 3)\n",
      "Adding  fold5 New Features:  (936, 101, 60, 3)\n",
      "Adding  fold6 New Features:  (823, 101, 60, 3)\n",
      "Adding  fold7 New Features:  (838, 101, 60, 3)\n",
      "Adding  fold8 New Features:  (806, 101, 60, 3)\n",
      "Scaling time: 2.2647957801818848\n",
      "Scaling time: 0.2746145725250244\n",
      "Scaling time: 0.3214297294616699\n",
      "training model...hold tight\n",
      "Train on 7074 samples, validate on 816 samples\n",
      "Epoch 1/150\n",
      "7074/7074 [==============================] - 17s - loss: 1.5484 - acc: 0.4757 - val_loss: 1.2522 - val_acc: 0.5748\n",
      "Epoch 2/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.9463 - acc: 0.6703 - val_loss: 1.1706 - val_acc: 0.5404\n",
      "Epoch 3/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.7224 - acc: 0.7567 - val_loss: 1.1026 - val_acc: 0.6029\n",
      "Epoch 4/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.5462 - acc: 0.8258 - val_loss: 1.0925 - val_acc: 0.6446\n",
      "Epoch 5/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.4225 - acc: 0.8709 - val_loss: 1.2019 - val_acc: 0.5968\n",
      "Epoch 6/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.3279 - acc: 0.9033 - val_loss: 1.2170 - val_acc: 0.6360\n",
      "Epoch 7/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.2367 - acc: 0.9370 - val_loss: 1.2671 - val_acc: 0.6250\n",
      "Epoch 8/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.1863 - acc: 0.9484 - val_loss: 1.3967 - val_acc: 0.6005\n",
      "Epoch 9/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.1253 - acc: 0.9734 - val_loss: 1.5559 - val_acc: 0.6434\n",
      "Epoch 10/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0937 - acc: 0.9819 - val_loss: 1.4874 - val_acc: 0.6360\n",
      "Epoch 11/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0701 - acc: 0.9876 - val_loss: 1.6866 - val_acc: 0.6483\n",
      "Epoch 12/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0519 - acc: 0.9914 - val_loss: 1.5677 - val_acc: 0.6520\n",
      "Epoch 13/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0397 - acc: 0.9945 - val_loss: 1.7512 - val_acc: 0.6348\n",
      "Epoch 14/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0331 - acc: 0.9959 - val_loss: 1.8475 - val_acc: 0.6605\n",
      "Epoch 15/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0289 - acc: 0.9956 - val_loss: 1.8284 - val_acc: 0.6630\n",
      "Epoch 16/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0223 - acc: 0.9967 - val_loss: 1.7778 - val_acc: 0.6618\n",
      "Epoch 17/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0207 - acc: 0.9966 - val_loss: 1.8544 - val_acc: 0.6630\n",
      "Epoch 18/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0176 - acc: 0.9970 - val_loss: 2.0435 - val_acc: 0.6360\n",
      "Epoch 19/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0212 - acc: 0.9959 - val_loss: 1.8557 - val_acc: 0.6654\n",
      "Epoch 20/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0127 - acc: 0.9983 - val_loss: 1.9824 - val_acc: 0.6618\n",
      "Epoch 21/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0140 - acc: 0.9979 - val_loss: 1.9703 - val_acc: 0.6654\n",
      "Epoch 22/150\n",
      "7074/7074 [==============================] - 14s - loss: 0.0122 - acc: 0.9977 - val_loss: 2.0078 - val_acc: 0.6593\n",
      "Epoch 23/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0114 - acc: 0.9979 - val_loss: 2.2167 - val_acc: 0.6348\n",
      "Epoch 24/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0127 - acc: 0.9970 - val_loss: 2.0182 - val_acc: 0.6495\n",
      "Epoch 25/150\n",
      "7074/7074 [==============================] - 13s - loss: 0.0110 - acc: 0.9977 - val_loss: 2.0990 - val_acc: 0.6605\n",
      "Epoch 00024: early stopping\n",
      "training time: 349.84084033966064\n",
      "800/837 [===========================>..] - ETA: 0s\n",
      "Accuracy = 0.6631\n",
      "\n",
      "Error Rate = 0.3369\n",
      "training time: 1.0011117458343506\n"
     ]
    }
   ],
   "source": [
    "# earlystopping ends training when the validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for test_fold in range(1, 11):\n",
    "    print('opening fold:', str(test_fold))\n",
    "    keras.backend.clear_session()\n",
    "    model = build_model()\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = load_all_folds(test_fold)\n",
    "\n",
    "    # for each channel, compute scaling factor\n",
    "    scaler_list = []\n",
    "    (n_clips, n_time, n_freq, n_channel) = train_x.shape\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        t1 = time.time()\n",
    "        xtrain_2d = train_x[:, :, :, channel].reshape((n_clips * n_time, n_freq))\n",
    "        scaler = sklearn.preprocessing.StandardScaler().fit(xtrain_2d)\n",
    "        # print(\"Channel %d Mean: %s\" % (channel, scaler.mean_,))\n",
    "        # print(\"Channel %d Std: %s\" % (channel, scaler.scale_,))\n",
    "        # print(\"Calculating scaler time: %s\" % (time.time() - t1,))\n",
    "        scaler_list += [scaler]\n",
    "\n",
    "    train_x = do_scale(train_x)\n",
    "    valid_x = do_scale(valid_x)\n",
    "    test_x = do_scale(test_x)\n",
    "\n",
    "    # use a batch size to fully utilize GPU power\n",
    "    t1 = time.time()\n",
    "    print(\"training model...hold tight\")\n",
    "    history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=callbacks,\n",
    "                        batch_size=40,\n",
    "                        epochs=150)\n",
    "    print(\"training time: %s\" % (time.time() - t1,))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    acc = evaluate(model, test_x, test_y)\n",
    "    print(\"training time: %s\" % (time.time() - t2,))\n",
    "\n",
    "    acc_list += [acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc mean 0.6271 acc std 0.0539\n"
     ]
    }
   ],
   "source": [
    "acc_array = np.array(acc_list)\n",
    "print(\"acc mean %.4f acc std %.4f\" % (acc_array.mean(), acc_array.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History keys: dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7MAAAHzCAYAAADy2UoqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGYlSZIkSW3HMCtJkiRJajv/H6COKDZKL0N2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c49871fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "print(\"History keys:\", (history.history.keys()))\n",
    "# summarise history for training and validation set accuracy\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarise history for training and validation set loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss', fontsize = 'large')\n",
    "plt.xlabel('epoch', fontsize = 'large' )\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:913: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/837 [===========================>..] - ETA: 0s\n",
      "Accuracy = 0.6631\n",
      "\n",
      "Error Rate = 0.3369\n",
      "Showing Confusion Matrix\n",
      "                    air conditioner            horn        children             dog           drill          engine             gun          hammer           siren           music \n",
      "    air conditioner              77               0               0               0               0              19               0               2               1               1 \n",
      "               horn               0              23               3               1               2               0               1               2               0               1 \n",
      "           children               1               0              65              12               1               2               0               0               7              12 \n",
      "                dog               1               6              15              62               6               1               0               4               2               3 \n",
      "              drill               0               0               4               6              65               4               0               7              13               1 \n",
      "             engine               1               0               5               1               1              78               0               5               1               1 \n",
      "                gun               0               3               0               3               0               0              26               0               0               0 \n",
      "             hammer               2               3               0               0              41               3               0              47               0               0 \n",
      "              siren               6               1              25               7               2               1               0               0              40               1 \n",
      "              music               1               0              14               0              10               0               0               0               3              72 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3c5ef50898>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[earlystop], batch_size=32, nb_epoch=50)\n",
    "acc = evaluate(model, test_x, test_y)  #evaluate(model)\n",
    "\n",
    "labels = [\"air conditioner\", \"horn\", \"children\", \"dog\", \"drill\", \"engine\", \"gun\", \"hammer\", \"siren\", \"music\"]\n",
    "print(\"Showing Confusion Matrix\")\n",
    "y_prob = model.predict(test_x, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=-1)\n",
    "y_true = np.argmax(test_y, 1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=' ')\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=' ')\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=' ')\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}s\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=' ')\n",
    "        print()\n",
    "\n",
    "print_cm(cm, labels)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, labels, labels)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g', linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
